# Figures and analysis code for A. Filipowicz, C. Glaze, J. Kable, & J. Gold (2019)

The following scripts contain the code used to generate the analysis and figures for the article "Pupil diameter encodes the idiosyncratic, cognitive complexity of belief updating".

Figures and analyses appear in the same order in which they appear in the text

Data involve large files and will be made available upon request.
Any questions should be referred to Alex Filipowicz: alsfilip[at]gmail.com

## 0) Load libraries and data files
```{r}
library(data.table)
library(lme4)
library(plyr)
library(tidyr)
library(ggplot2)
library(lmerTest)
library(PMCMRplus)
library(lmtest)
library(parallel)

# Root directory
root = '/Users/alsfilip/Dropbox/Penn/Auditory_2AFC/Auditory_2AFC/Figures/Final_Figures/'

#Load pupil data
pdat = fread(paste0(root,'/data/pdat.csv'),sep=',')
#Load behavioral data (includes subject for whom we did not have any pupil data)
bdat = read.csv(paste0(root,'data/bdat.csv'),sep=',')
#Load model fits and other subject information
adaptH_all = read.csv(paste0(root,'data/adaptH_all.csv'),sep=',')

#Color scheme for different hazard rates
hcol = c("#0072B2","#D55E00","#E69F00")
```


## FIGURE 1

### Figure 1.b - trial sequence
```{r}
#Trials
ex_trials_dt = pdat[1:3000,.(Hazard,Source,Stim)]
ex_trials_dt[,("Hazard") := as.numeric(ex_trials_dt$Hazard)]
ex_trials_dt$Trial = 1:3000
ex_trials_dt.long = gather(ex_trials_dt,Type,Value,Hazard:Stim)

ex_trials_plt = ggplot(ex_trials_dt.long,aes(Trial,Value))+
  geom_line()+
  geom_vline(xintercept=c(1000,2000),linetype=2,color='red')+
  facet_wrap(~Type,nrow=3)+
  xlab('Trial')+
  theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave('Figure1/ex_trials_plt.pdf',ex_trials_plt,height = 5,width = 8)
```

### Figure 1.c-f - subject and simulated proportion correct responses and proportion prediction switches
```{r}
# Get proportion correct and prediction switch by hazard rate for trials after source switch
bdat.h.tscp.p = ddply(bdat,.(Subject,H,TSCP2),summarize,
                        MCor = mean(Correct,na.rm=T),
                        MRSw = mean(RespSwitch,na.rm=T))

#Proportion Correct
bdat.h.tscp.cor = ddply(bdat.h.tscp.p,.(H,TSCP2),summarize,
                        Value = mean(MCor,na.rm=T),
                        MSE = mean_se(MCor)[[2]],
                        PSE = mean_se(MCor)[[3]])
bdat.h.tscp.cor$Hazard = factor(bdat.h.tscp.cor$H,labels=c('H = 0.01','H = 0.3','H = 0.99')) 

#Prediction (response) switch
bdat.h.tscp.rsw = ddply(bdat.h.tscp.p,.(H,TSCP2),summarize,
                        Value = mean(MRSw,na.rm=T),
                        MSE = mean_se(MRSw)[[2]],
                        PSE = mean_se(MRSw)[[3]])
bdat.h.tscp.rsw$Hazard = factor(bdat.h.tscp.rsw$H,labels=c('H = 0.01','H = 0.3','H = 0.99'))

behavPlt = function(dat,yl,sim=F){
  if(sim == F){
    title = 'Subject Behavior'
    lt = 1
  }
  else{
    title = 'Simulated Behavior'
    lt = 2
  }
  plt = ggplot(dat,aes(TSCP2,Value,ymin=MSE,ymax=PSE,color=Hazard,group=Hazard))+
    geom_errorbar(width=.1)+
    geom_line(linetype=lt)+
    geom_point(size=3)+
    ylab(yl)+
    xlab('Trial Since Source Switch')+
    ggtitle(title)+
    scale_color_manual(values=hcol)+
    theme(
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
      )
  return(plt)
}

#Subject Behavior
sub_behav_corr_plt = behavPlt(bdat.h.tscp.cor,'Proportion Correct Responses')
ggsave('Figure1/sub_behav_corr.pdf',sub_behav_corr_plt,height = 4,width = 6)
sub_behav_rsw_plt = behavPlt(bdat.h.tscp.rsw,'Proportion Prediction Switches')
ggsave('Figure1/sub_behav_rsw.pdf',sub_behav_rsw_plt,height = 4,width = 6)


# Model behavior
sim.h.tscp.p = ddply(bdat,.(Subject,H,TSCP2),summarize,
                        MCor = mean(Bayes_Prior_Correct,na.rm=T),
                        MRSw = mean(Bayes_Prior_Resp_Switch,na.rm=T))

sim.h.tscp.cor = ddply(sim.h.tscp.p,.(H,TSCP2),summarize,
                        Value = mean(MCor,na.rm=T),
                        MSE = mean_se(MCor)[[2]],
                        PSE = mean_se(MCor)[[3]])
sim.h.tscp.cor$Hazard = factor(sim.h.tscp.cor$H,labels=c('H = 0.01','H = 0.3','H = 0.99'))

sim.h.tscp.rsw = ddply(sim.h.tscp.p,.(H,TSCP2),summarize,
                        Value = mean(MRSw,na.rm=T),
                        MSE = mean_se(MRSw)[[2]],
                        PSE = mean_se(MRSw)[[3]])
sim.h.tscp.rsw$Hazard = factor(sim.h.tscp.rsw$H,labels=c('H = 0.01','H = 0.3','H = 0.99'))

#Simulated Behavior
sim_behav_corr_plt = behavPlt(sim.h.tscp.cor,'Proportion Correct Responses',sim=T)
ggsave('Figure1/sim_behav_corr.pdf',sim_behav_corr_plt,height = 4,width = 6)
sim_behav_rsw_plt = behavPlt(sim.h.tscp.rsw,'Proportion Prediction Switches',sim=T)
ggsave('Figure1/sim_behav_rsw.pdf',sim_behav_rsw_plt,height = 4,width = 6)
```

```{r}
#PROPORTION CORRECT
#Summary stats
bdat.h.p = ddply(bdat,.(Subject,H),summarize,MCorr = mean(Correct,na.rm=T))
bdat.h.m = ddply(bdat.h.p,.(H),summarize,MedCorr = median(MCorr,na.rm=T),IQ1 = quantile(MCorr,na.rm=T)[[2]],IQ2 = quantile(MCorr,na.rm=T)[[4]])
print(bdat.h.m)

# Stats comparing proportion correct responses in each hazard rate
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .01],bdat.h.p$MCorr[bdat.h.p$H == .3],paired=T)
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .01],bdat.h.p$MCorr[bdat.h.p$H == .99],paired=T)
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .3],bdat.h.p$MCorr[bdat.h.p$H == .99],paired=T)

# Stats comparing proportion correct responses in each hazard rate
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .01],mu=.5)
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .3],mu=.5)
wilcox.test(bdat.h.p$MCorr[bdat.h.p$H == .99],mu=.5)

#DIFFERENCES IN PUPIL BY HAZARD
pdat.pupil = pdat[,.(Subject,Hazard,Baseline,Baseline_1,Change,Change_1)]
pdat.pupil[,("Baseline") := scale(Baseline)]
pdat.pupil[,("Baseline_1") := scale(Baseline_1)]
pdat.pupil[,("Change") := scale(Change)]
pdat.pupil[,("Change_1") := scale(Change_1)]
pdat.pupil[,("Hazard") := factor(Hazard,labels=c('H = 0.01','H = 0.3', 'H = .99'))]

#BASELINE CONTRASTS
base.contrast.h1 = lmer(Baseline~Hazard+Baseline_1+Change_1+
                          (1|Subject)+(1+Hazard|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                        pdat.pupil,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
summary(base.contrast.h1)

pdat.pupil[,("Hazard") := factor(Hazard,levels=c('H = 0.3','H = 0.01', 'H = .99'))]
base.contrast.h3 = lmer(Baseline~Hazard+Baseline_1+Change_1+
                          (1|Subject)+(1+Hazard|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                        pdat.pupil,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
summary(base.contrast.h3)

#EVOKED CHANGE CONTRASTS
pdat.pupil[,("Hazard") := factor(Hazard,levels=c('H = 0.01','H = 0.3', 'H = .99'))]
ec.contrast.h1 = lmer(Change~Hazard+Baseline+Baseline_1+Change_1+
                          (1|Subject)+(1+Hazard|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                        pdat.pupil,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
summary(ec.contrast.h1)

pdat.pupil[,("Hazard") := factor(Hazard,levels=c('H = 0.3','H = 0.01', 'H = .99'))]
ec.contrast.h3 = lmer(Change~Hazard+Baseline+Baseline_1+Change_1+
                          (1|Subject)+(1+Hazard|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                        pdat.pupil,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
summary(ec.contrast.h3)
```






## FIGURE 2
### Figure 2a,c - influence of stimulus and prediction (response) switches baseline and next baseline pupil for each hazard conditon
# Analysis
```{r}
#Function to compute influence switches on baseline pupil (both current baseline and next baseline)
getBaseLME = function(dat,h,nextB = F){
  d = subset(dat,Hazard == h)
  d$StimSwitch = scale(d$StimSwitch)
  d$RespSwitch = scale(d$RespSwitch)
  d$Baseline = scale(d$Baseline)
  d$Baseline_1 = scale(d$Baseline_1)
  d$Change_1 = scale(d$Change_1)
  
  #For current baseline
  if(nextB == F){
    d.lme = lmer(Baseline~StimSwitch*RespSwitch+Baseline_1+Change_1+
                   (1|Subject)+(1+StimSwitch*RespSwitch|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),d,
                 REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  }
  else{
    d.lme = lmer(Baseline_plus_1~StimSwitch*RespSwitch+Baseline+Baseline_1+Change_1+
                   (1|Subject)+(1+StimSwitch*RespSwitch|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),d,
                 REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  }
  summ = summary(d.lme)
  df = coef(d.lme)$Subject
  df$Subject = row.names(df)
  return(list(DF = df,SUMM = summ))
}

#Paralellize process - takes about 3.5 minutes total on a 2017 quad-core macbook pro
pdat.base = pdat[,.(Subject,Hazard,StimSwitch,RespSwitch,Baseline,Baseline_1,Change_1,Baseline_plus_1)]
library(parallel)
ncores = 3
cl = makeCluster(ncores)
clusterExport(cl = cl,varlist = c('getBaseLME','pdat.base','lmer','lmerControl','scale'))
base.rsw.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME(pdat.base,h))
saveRDS(base.rsw.lmes,'./rds/base_rsw_lmes.rds')
base1.rsw.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME(pdat.base,h,nextB=T))
saveRDS(base1.rsw.lmes,'./rds/base1_rsw_lmes.rds')
stopCluster(cl)
```
# Extract computed beta weights and significance values and plot
```{r}
base.rsw.lmes = readRDS('./rds/base_rsw_lmes.rds')
base1.rsw.lmes = readRDS('./rds/base1_rsw_lmes.rds')

getBaseBetas = function(lmes){
  hs = c('H = 0.01','H = 0.3','H = 0.99')
  df = lmes[[1]]$DF
  df$Hazard = hs[1]
  print(hs[1])
  print(lmes[[1]]$SUMM$coefficients)
  for(i in 2:3){
    dfp = lmes[[i]]$DF
    dfp$Hazard = hs[i]
    df = rbind(df,dfp)
    print(hs[i])
    print(lmes[[i]]$SUMM$coefficients)
  }
  return(df)
}

plotBaseBetas=function(df,title){
  #Set data frame up to plot betas for StimSwitch and RespSwitch
  df.long = gather(df,Type,Beta,RespSwitch:StimSwitch)
  df.long$Type = factor(df.long$Type,labels=c('Pred','Stim'))
  df.long.m = ddply(df.long,.(Hazard,Type),summarize,MBeta = mean(Beta,na.rm=T),MSE = mean_se(Beta)[[2]],PSE = mean_se(Beta)[[3]])
  print(df.long.m)
  
  #Plot
  plt = #ggplot(df.long,aes(Type,Beta,ymin=MSE,ymax=PSE,color=Hazard,fill=Hazard))+
    ggplot(df.long,aes(Type,Beta,color=Hazard,fill=Hazard))+
    geom_hline(yintercept=c(0),linetype=3)+
    #geom_bar(stat='identity',aes(alpha=Type))+
    #geom_errorbar(width=0,size=2,color='black')+
    geom_boxplot(aes(alpha=Type))+
    #geom_point(aes(alpha=Type),shape=16,stroke=.75,position = position_jitter(w = 0.1, h = 0))+
    facet_wrap(~Hazard,ncol=3)+
    scale_color_manual(values=hcol)+
    scale_fill_manual(values=hcol)+
    scale_alpha_manual(values=c(.8,.3))+
    #ggtitle(title)+
    ylab('Beta Weights')+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black',vjust=.5,hjust=0,angle=315),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
  return(plt)
}

base.beta = getBaseBetas(base.rsw.lmes)
base.beta.sw.plt = plotBaseBetas(base.beta[,c(7,8,3,2)],'Baseline')
ggsave('Figure2/base_sw_betas.pdf',base.beta.sw.plt,height = 5.2,width = 4.5)

base1.beta = getBaseBetas(base1.rsw.lmes)
base1.beta.sw.plt = plotBaseBetas(base1.beta[,c(8,9,3,2)],'Next Baseline')
ggsave('Figure2/base1_sw_betas.pdf',base1.beta.sw.plt,height = 5.2,width = 4.5)
```


### Figure 2b - influnce of switches on evoked changes at 120 different time points after tone onset
```{r}
#Function to compute influence of switches on pupil samples after tone onset
getECLME = function(dat,h){
  d = subset(dat,Hazard == h)
  names(d)[8] = 'Pupil'
  d$PupilDiff = d$Pupil-d$Baseline
  d$PupilDiff = scale(d$PupilDiff)
  d$StimSwitch = scale(d$StimSwitch)
  d$RespSwitch = scale(d$RespSwitch)
  d$Baseline = scale(d$Baseline)
  d$Baseline_1 = scale(d$Baseline_1)
  d$Change_1 = scale(d$Change_1)
  
  d.lme = lmer(PupilDiff~StimSwitch*RespSwitch+Baseline+Baseline_1+Change_1+
                  (1|Subject)+(1+StimSwitch*RespSwitch|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),d,
                REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  summ = summary(d.lme)
  df = coef(d.lme)$Subject
  df$Subject = row.names(df)
  return(list(DF = df,SUMM = summ))
}

#Paralellize process - takes about 5 hours on a 2017 quadcore macbook pro
pdat.ec = pdat[,c(3,5,10,12,141,144,146,21:140)]
library(parallel)
ncores = 4
cl = makeCluster(ncores)
clusterExport(cl = cl,varlist = c('getECLME','pdat.ec','lmer','lmerControl','scale'))
ec.rsw.h1.lmes = parLapply(cl,1:4, function(x) getECLME(pdat.ec[c(1:7,7+x)],.01))
saveRDS(ec.rsw.h1.lmes,'./rds/ec_rsw_h1_lmes.rds')
ec.rsw.h3.lmes = parLapply(cl,1:4, function(x) getECLME(pdat.ec[c(1:7,7+x)],.3))
saveRDS(ec.rsw.h3.lmes,'./rds/ec_rsw_h3_lmes.rds')
ec.rsw.h9.lmes = parLapply(cl,1:4, function(x) getECLME(pdat.ec[c(1:7,7+x)],.99))
saveRDS(ec.rsw.h9.lmes,'./rds/ec_rsw_h9_lmes.rds')
stopCluster(cl)
```
# Load beta weights and significance values
```{r}
ec.rsw.h1.lmes = readRDS(',/rds/ec_rsw_h1_lmes.rds')
ec.rsw.h3.lmes = readRDS('./rds/ec_rsw_h3_lmes.rds')
ec.rsw.h9.lmes = readRDS('./rds/ec_rsw_h9_lmes.rds')

#Compare to previously computed 
root2 = '/Users/alsfilip/Dropbox/Penn/Auditory_2AFC/Auditory_2AFC/'
h1.sample.sw.rsw.name = 'H1Samples_lme_lowpass_eyeres.rds' #eyeres means pupil data with eye position regressed out
h3.sample.sw.rsw.name = 'H3Samples_lme_lowpass_eyeres.rds' #eyeres means pupil data with eye position regressed out
h9.sample.sw.rsw.name = 'H9Samples_lme_lowpass_eyeres.rds' #eyeres means pupil data with eye position regressed out
h1.sw.rsw.samples = readRDS(paste0(root2,'Pupil_Processed/R_Objects/',h1.sample.sw.rsw.name))
h3.sw.rsw.samples = readRDS(paste0(root2,'Pupil_Processed/R_Objects/',h3.sample.sw.rsw.name))
h9.sw.rsw.samples = readRDS(paste0(root2,'Pupil_Processed/R_Objects/',h9.sample.sw.rsw.name))
```














## FIGURE 3
### Figure 3a,c - influence of belief strength and surprise on baseline and next baseline - also generates analyses and plots for S2a,c

Analysis
```{r}
#Function to compute influence switches on baseline pupil (both current baseline and next baseline)
getBaseLME2 = function(dat,h,nextB = F,rt = F){
  d = subset(dat,Hazard == h)
  d$L_Bayes_Prior_Ab = scale(d$L_Bayes_Prior_Ab)
  d$Surprise = scale(d$Surprise)
  d$Baseline = scale(d$Baseline)
  d$Baseline_1 = scale(d$Baseline_1)
  d$Change_1 = scale(d$Change_1)
  d$logRT2 = scale(d$logRT2)
  d$NextlogRT2 = scale(d$NextlogRT2)
  
  if(rt == F){
    #For current baseline
    if(nextB == F){
      d.lme = lmer(Baseline~L_Bayes_Prior_Ab+Surprise+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+Change_1|Subject),d,
                   REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
    }
    #For next baseline
    else{
      d.lme = lmer(Baseline_plus_1~L_Bayes_Prior_Ab+Surprise+Baseline+Baseline_1+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),d,
                   REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
    }
  }
  else{
    #For current baseline - accounting for logRT
    if(nextB == F){
      d.lme = lmer(Baseline~L_Bayes_Prior_Ab+Surprise+logRT2+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+logRT2|Subject)+(1+Change_1|Subject),d,
                   REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
    }
    #For next baseline - accounting for next log RT
    else{
      d.lme = lmer(Baseline_plus_1~L_Bayes_Prior_Ab+Surprise+NextlogRT2+Baseline+Baseline_1+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+NextlogRT2|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),d,
                   REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
    }   
  }
  summ = summary(d.lme)
  df = coef(d.lme)$Subject
  df$Subject = row.names(df)
  df$Hazard = paste0('H = ',h)
  return(list(DF = df,SUMM = summ))
}


#Paralellize process - takes about 3 minutes total on a 2017 quad-core macbook pro
pdat.base2 = pdat[,.(Subject,Hazard,L_Bayes_Prior_Ab,Surprise,Baseline,Baseline_1,Change_1,Baseline_plus_1,logRT2,NextlogRT2)]
library(parallel)
ncores = 3
cl = makeCluster(ncores)
clusterExport(cl = cl,varlist = c('getBaseLME2','pdat.base2','lmer','lmerControl','scale'))

#Baseline vs belief strength and surprise
base.prior.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME2(pdat.base2,h))
saveRDS(base.prior.lmes,'./rds/base_prior_lmes.rds')
#Accounting for RTs
base.prior.rt.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME2(pdat.base2,h,rt=T))
saveRDS(base.prior.rt.lmes,'./rds/base_prior_rt_lmes.rds')

#Next Baseline vs belief strength and surprise
base1.prior.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME2(pdat.base2,h,nextB = T))
saveRDS(base1.prior.lmes,'./rds/base1_prior_lmes.rds')
#Accounting for RTs
base1.prior.rt.lmes = parLapply(cl,c(.01,.3,.99), function(h) getBaseLME2(pdat.base2,h,nextB = T,rt=T))
saveRDS(base1.prior.rt.lmes,'./rds/base1_prior_rt_lmes.rds')
stopCluster(cl)
```

Plotting
```{r}
base.prior.lmes = readRDS('./rds/base_prior_lmes.rds')
base.prior.betas = rbind(base.prior.lmes[[1]]$DF,base.prior.lmes[[2]]$DF,base.prior.lmes[[3]]$DF)

base.prior.rt.lmes = readRDS('./rds/base_prior_rt_lmes.rds')
base.prior.rt.betas = rbind(base.prior.rt.lmes[[1]]$DF,base.prior.rt.lmes[[2]]$DF,base.prior.rt.lmes[[3]]$DF)

base1.prior.lmes = readRDS('./rds/base1_prior_lmes.rds')
base1.prior.betas = rbind(base1.prior.lmes[[1]]$DF,base1.prior.lmes[[2]]$DF,base1.prior.lmes[[3]]$DF)

base1.prior.rt.lmes = readRDS('./rds/base1_prior_rt_lmes.rds')
base1.prior.rt.betas = rbind(base1.prior.rt.lmes[[1]]$DF,base1.prior.rt.lmes[[2]]$DF,base1.prior.rt.lmes[[3]]$DF)

#Function to display stats and plot betas
plotBaseBetas2 = function(df,lmes,title){
  #Print regression betas and p-values
  hs = c('H = .01','H = .3','H = .99')
  for(i in 1:length(hs)){
    print(hs[i])
    print(lmes[[i]]$SUMM$coefficients)
  }
  
  #Plot betas
  df.long = gather(df,Type,Beta,L_Bayes_Prior_Ab:Surprise)
  df.long$Type = factor(df.long$Type,labels=c('Subject Absolute\nBelief Strength','Subject Surprise'))
  df.long.m = ddply(df.long,.(Hazard,Type),summarize,MBeta = mean(Beta,na.rm=T),MSE = mean_se(Beta)[[2]],PSE=mean_se(Beta)[[3]])
  
  plt = ggplot(df.long,aes(Type,Beta,color=Hazard,fill=Hazard))+
  #ggplot(df.long.m,aes(Type,MBeta,ymin=MSE,ymax=PSE,color=Hazard,fill=Hazard))+
    geom_hline(yintercept=c(0),linetype=3)+
    geom_boxplot(aes(alpha=Type))+
    #geom_bar(stat='identity',aes(alpha=Type))+
    #geom_errorbar(width=0,size=2,color='black')+
    facet_wrap(~Hazard,ncol=3)+
    scale_color_manual(values=hcol)+
    scale_fill_manual(values=hcol)+
    scale_alpha_manual(values=c(.8,.3))+
    #ggtitle(title)+
    ylab('Beta Weights')+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black',angle=315,vjust=1,hjust=0),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
  return(plt)
}

# Plots
base.prior.plt = plotBaseBetas2(base.prior.betas,base.prior.lmes,'Baseline')
ggsave('Figure3/base_prior_betas.pdf',base.prior.plt,height = 6,width = 4.5)
base1.prior.plt = plotBaseBetas2(base1.prior.betas,base1.prior.lmes,'Next Baseline')
ggsave('Figure3/base1_prior_betas.pdf',base1.prior.plt,height = 6,width = 4.5)

#Accounting for RTs
base.prior.plt.rt = plotBaseBetas2(base.prior.rt.betas,base.prior.rt.lmes,'Baseline\naccounting for log(RT)')
ggsave('FigureS2/base_prior_betas_rt.pdf',base.prior.plt.rt,height = 6,width = 4.5)
base1.prior.plt.rt = plotBaseBetas2(base1.prior.rt.betas,base1.prior.rt.lmes,'Next Baseline\naccounting for Next log(RT)')
ggsave('FigureS2/base1_prior_betas_rt.pdf',base1.prior.plt.rt,height = 6,width = 4.5)
```



### Figure 3b - influence of model parameters on evoked change - also generates S2b
```{r}
#Get post-tone pupil samples
pdat_ec2 = pdat[,c(3,5,199,204,141,144,146,211,21:140)]
pdat_ec2[,("Hazard"):= factor(pdat_ec$Hazard,labels=c("H = .01","H = .3","H = .99"))]
pdat_ec2 = data.frame(pdat_ec)

#Function to compute beta for each sample point after stimulus onset
getPupBeta = function(d,x,h,rt = F){
  dat = d[,c(1:8,x+8)]
  names(dat)[9] = 'Pupil'
  dat$PupilDiff = dat$Pupil-dat$Baseline
  dat$L_Bayes_Prior_Ab = scale(dat$L_Bayes_Prior_Ab)
  dat$Surprise=scale(dat$Surprise)
  dat$PupilDiff= scale(dat$Pupil)
  dat$Baseline= scale(dat$Baseline)
  dat$Baseline_1=scale(dat$Baseline_1)
  dat$Change_1=scale(dat$Change_1)
  if(rt = F){
    dat_ec_lme = lmer(PupilDiff~L_Bayes_Prior_Ab+Surprise+Baseline+Baseline_1+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),dat,
                    REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  }
  else{
    dat_ec_lme = lmer(PupilDiff~L_Bayes_Prior_Ab+Surprise+NextlogRT2+Baseline+Baseline_1+Change_1+
                     (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+NextlogRT2|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),dat,
                    REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))    
  }
  df = coef(dat_ec_lme)
  return(list(DF=df$Subject,SUMM=summary(dat_ec_lme)))
}

#Takes about 9 hours to run on a 2017 quad-core macbook pro
library(parallel)
cl = makeCluster(8)
clusterExport(cl = cl,varlist = c('getPupBeta','pdat_ec','lmer','lmerControl','coef','scale'))
h1_ec_bel = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .01',],x,'H = .01',rt = F))
saveRDS(object = h1_ec_bel,"h1_ec_bel.rds")
h1_ec_bel_rt = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .01',],x,'H = .01',rt = T))
saveRDS(object = h1_ec_bel_rt,"h1_ec_bel_rt.rds")

h3_ec_bel = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .3',],x,'H = .3',rt = F))
saveRDS(object = h3_ec_bel,"h3_ec_bel.rds")
h3_ec_bel_rt = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .3',],x,'H = .3',rt = T))
saveRDS(object = h3_ec_bel_rt,"h3_ec_bel_rt.rds")

h9_ec_bel_rt = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .99',],x,'H = .99',rt = F))
saveRDS(object = h9_ec_bel_rt,"h9_ec_bel.rds")
h9_ec_bel = parLapply(cl,1:120,function(x) getPupBeta(pdat_ec[pdat_ec$Hazard == 'H = .99',],x,'H = .99',rt = T))
saveRDS(object = h9_ec_bel,"h9_ec_bel_rt.rds")
stopCluster(cl)
```

Figure 3d,e - influence of model values on RTs
```{r}
#REACTION TIMES
bdat$Surprise = ifelse(bdat$X == 2,-log(bdat$P_Bayes_Prior),-log(1-bdat$P_Bayes_Prior))
bdat$Subject = factor(bdat$Subject)
bdat = data.table(bdat)

getLogRTBetas = function(dat,h){
  h_dat = dat[H == h]
  h_dat[,("logRT2") := scale(h_dat$logRT2)]
  h_dat[,("NextlogRT2") := scale(h_dat$NextlogRT2)]
  h_dat[,("L_Bayes_Prior_Ab") := scale(h_dat$L_Bayes_Prior_Ab)]
  h_dat[,("Surprise") := scale(h_dat$Surprise)]
  h_logRT_lme = lmer(logRT2~L_Bayes_Prior_Ab+Surprise+(1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject),h_dat,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  h_logRT_df = coef(h_logRT_lme)$Subject
  h_logRT_df$Subject = row.names(h_logRT_df)
  h_logRT_df$Hazard = h
  h_logRT_df$Time = 'logRT'
  print(summary(h_logRT_lme))
  
  h_NextlogRT_lme = lmer(NextlogRT2~L_Bayes_Prior_Ab+Surprise+(1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject),h_dat,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  h_NlogRT_df = coef(h_NextlogRT_lme)$Subject
  h_NlogRT_df$Subject = row.names(h_NlogRT_df)
  h_NlogRT_df$Hazard = h
  h_NlogRT_df$Time = 'NextlogRT'
  print(summary(h_NextlogRT_lme))
  
  return(rbind(h_logRT_df,h_NlogRT_df))
}

h1_rt_df = getLogRTBetas(bdat,.01)
h3_rt_df = getLogRTBetas(bdat,.3)
h9_rt_df = getLogRTBetas(bdat,.99)

h_rt_df = rbind(h1_rt_df,h3_rt_df,h9_rt_df)
h_rt_df.long = gather(h_rt_df,Type,Beta,L_Bayes_Prior_Ab:Surprise)
h_rt_df.long$Hazard = factor(h_rt_df.long$Hazard,labels=c('H = .01','H = .3','H = .99'))
h_rt_df.long$Type = factor(h_rt_df.long$Type,labels=c('Subject Absolute\nBelief Strength','Subject\nSurprise'))
h_rt_df.long.m = ddply(h_rt_df.long,.(Time,Type,Hazard),summarize,Mbeta=mean(Beta),mse=mean_se(Beta)[[2]],pse=mean_se(Beta)[[3]])
h_rt_df.long.m$Hazard = factor(h_rt_df.long.m$Hazard,labels=c('H = .01','H = .3','H = .99'))
h_rt_df.long.m$Type = factor(h_rt_df.long.m$Type,labels=c('Subject Absolute\nBelief Strength','Subject\nSurprise'))

lrt = subset(h_rt_df.long,Time == 'logRT')
lrt_plot=ggplot(lrt,aes(Type,Beta,color=Hazard,fill=Hazard))+
  #ggplot(lrt,aes(Type,Mbeta,ymin=mse,ymax=pse,color=Hazard,fill=Hazard))+
  geom_hline(yintercept=c(0),linetype=3)+
  # geom_bar(aes(alpha = Type),stat='identity')+
  # geom_errorbar(color='black',size=2,width=0)+
  geom_boxplot(aes(alpha=Type))+
  scale_alpha_manual(values=c(.8,.3))+
  scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  scale_fill_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  facet_wrap(~Hazard)+
  ylab('Beta Weights')+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black',angle=315,hjust=0,vjust=1),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave('./Figure3/lrt_bel_h.pdf',plot = lrt_plot,height = 6,width = 5)

nlrt = subset(h_rt_df.long,Time == 'NextlogRT')
nlrt_plot = ggplot(nlrt,aes(Type,Beta,color=Hazard,fill=Hazard))+
  geom_hline(yintercept=c(0),linetype=3)+
  # geom_bar(aes(alpha = Type),stat='identity')+
  # geom_errorbar(color='black',size=2,width=0)+
  geom_boxplot(aes(alpha=Type))+
  scale_alpha_manual(values=c(.8,.3))+
  scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  scale_fill_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  facet_wrap(~Hazard)+
  ylab('Beta Weights')+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black',angle=315,hjust=0,vjust=1),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_blank(),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face="bold",hjust=.5),
      plot.title = element_text(hjust=.5,size=18,face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave('./Figure3/nlrt_bel_h.pdf',plot = nlrt_plot,height = 6,width = 5)
```
RT stats
```{r}
bdat$RT = exp(bdat$logRT2)
bdat.rt = ddply(bdat,.(Subject,H),summarize,MedRT = median(RT,na.rm=T))
bdat.rt.m = ddply(bdat.rt,.(H),summarize,RT = median(MedRT,na.rm=T),IQ1 = quantile(MedRT,na.rm=T)[[2]],IQ2 = quantile(MedRT,na.rm=T)[[4]])
print(bdat.rt.m)

#Compare median RTs
wilcox.test(bdat.rt$MedRT[bdat.rt$H == .01],bdat.rt$MedRT[bdat.rt$H == .3],paired=T)
wilcox.test(bdat.rt$MedRT[bdat.rt$H == .01],bdat.rt$MedRT[bdat.rt$H == .99],paired=T)
wilcox.test(bdat.rt$MedRT[bdat.rt$H == .3],bdat.rt$MedRT[bdat.rt$H == .99],paired=T)
```


#Get significance values for Fig 2b and 3b
```{r}
# Get 10 bins of pupil diameter after tone onset - used for significance testing in Fig 2b and 3b
#Get p-values: separate into 10 equal time bins after tone onset
pdat_ec = pdat[,c(3,5,10,12,199,204,141,144,146,211,21:140)]
pdat_ec[,("Hazard"):= factor(pdat_ec$Hazard,labels=c("H = .01","H = .3","H = .99"))]
pdat_ec = data.frame(pdat_ec)
pdat_ec[,c(131:140)] = NA
names(pdat_ec)[131:140] = sapply(1:10,function(x) paste0('T',x))

#Separate 120 samples into 10 time bins after tone onset and baseline-subtract
intervals = seq(11,130,120/10)
base = pdat_ec$Baseline

for(i in 1:length(intervals)){
  ec = pdat_ec[,intervals[i]:(intervals[i]+11)]
  ec_base_sub = ec-base
  pdat_ec[,130+i] = rowMeans(ec_base_sub,na.rm=T)
}

#Function to compute p-values for stim/prediction switches
getECLME_switch_bin = function(dat,h){
  d = subset(dat,Hazard == h)
  names(d)[11] = "PupilDiff"
  d$PupilDiff = scale(d$PupilDiff)
  d$Baseline = scale(d$Baseline)
  d$Baseline_1 = scale(d$Baseline_1)
  d$Change_1 = scale(d$Change_1)
  d$StimSwitch = scale(d$StimSwitch)
  d$RespSwitch = scale(d$RespSwitch)
  d$NextlogRT2 = scale(d$NextlogRT2)

  d.lme = lmer(PupilDiff~RespSwitch*StimSwitch+Baseline+Baseline_1+Change_1+
                   (1|Subject)+(1+RespSwitch*StimSwitch|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                 d,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  return(summary(d.lme))
}


#Function to compute p-values
getECLME_prior_bin = function(dat,h,rt = F){
  d = subset(dat,Hazard == h)
  names(d)[11] = "PupilDiff"
  d$PupilDiff = scale(d$PupilDiff)
  d$Baseline = scale(d$Baseline)
  d$Baseline_1 = scale(d$Baseline_1)
  d$Change_1 = scale(d$Change_1)
  d$L_Bayes_Prior_Ab = scale(d$L_Bayes_Prior_Ab)
  d$Surprise = scale(d$Surprise)
  d$NextlogRT2 = scale(d$NextlogRT2)
  
  if(rt == F){
    d.lme = lmer(PupilDiff~L_Bayes_Prior_Ab+Surprise+Baseline+Baseline_1+Change_1+
                   (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject),
                 d,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  }
  else{
      d.lme = lmer(PupilDiff~L_Bayes_Prior_Ab+Surprise+Baseline+Baseline_1+Change_1+NextlogRT2+
                  (1|Subject)+(1+L_Bayes_Prior_Ab|Subject)+(1+Surprise|Subject)+(1+Baseline|Subject)+(1+Baseline_1|Subject)+(1+Change_1|Subject)+(1+NextlogRT2|Subject),
                d,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
  }
  return(summary(d.lme))
}



#Compute p-values for 10 equal bins of baseline-subtracted pupil samples after tone onset
#Takes about 10 minutes total
library(parallel)
cl = makeCluster(8)
clusterExport(cl = cl,varlist = c('getECLME_prior_bin','getECLME_switch_bin','pdat_ec','lmer','lmerControl','summary','scale'))

#Without accounting for RTs
h1_ec_prior_ps = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .01'))
saveRDS(h1_ec_prior_ps,'h1_ec_prior_ps.rds')
h3_ec_prior_ps = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .3'))
saveRDS(h3_ec_prior_ps,'h3_ec_prior_ps.rds')
h9_ec_prior_ps = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .99'))
saveRDS(h9_ec_prior_ps,'h9_ec_prior_ps.rds')

#Accounting for RTs
h1_ec_prior_ps_rt = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .01',rt=T))
saveRDS(h1_ec_prior_ps_rt,'h1_ec_prior_ps_rt.rds')
h3_ec_prior_ps_rt = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .3',rt=T))
saveRDS(h3_ec_prior_ps,'h3_ec_prior_ps_rt.rds')
h9_ec_prior_ps_rt = parLapply(cl,1:length(intervals),function(x) getECLME_prior_bin(pdat_ec[,c(1:10,(130+x))],'H = .99',rt=T))
saveRDS(h9_ec_prior_ps_rt,'h9_ec_prior_ps_rt.rds')
stopCluster(cl)
```


```{r}

```







## FIGURE 4
Correlations between descriptive adaptivity model and complexity
```{r}
#Plots and print correlations for high and low variability subjects
pltCor = function(df,xl,yl){
  names(df)[2:3] = c('X','Y')

  #Correlations
  print(paste0('Correlations between ',xl,' and ',yl))
  print('Low Variability')
  print(cor.test(df$X[df$Variability_Group == 'Low Variability'],df$Y[df$Variability_Group == 'Low Variability'],method='spearman'))
  print('High Variability')
  print(cor.test(df$X[df$Variability_Group == 'High Variability'],df$Y[df$Variability_Group == 'High Variability'],method='spearman'))
  
  #Plot
  ggplot(df,aes(X,Y))+
    geom_point(size=4,alpha=.1)+
    geom_point(aes(shape=Variability_Group),size = 4,color='black')+
    geom_smooth(aes(linetype=Variability_Group),method='lm',color='black',se=F)+
    scale_shape_manual(values = c(1,16))+
    scale_linetype_manual(values=c(2,1))+
    ylab(yl)+
    xlab(xl)+
    theme(
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title.y = element_text(size=14,color='black'),
      axis.title.x = element_text(size=14,color='black'),
      strip.background = element_blank(),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
}

#Correlations and plots
pltCor(adaptH_all[,c(2,7,8,114)],'Adaptivity','Choice Variability')
pltCor(adaptH_all[,c(2,3,7,114)],'Complexity','Adaptivity')
pltCor(adaptH_all[,c(2,3,8,114)],'Complexity','Choice Variability')

```







## FIGURE 5
Figure 5a - influence of past stimuli on current responses

Analysis - compute influence of previous stimuli on current response
```{r}
#Compute influence of previous trials on current response for each hazard rate
sBack_glme = function(H,a_all,dat){

  #sdat = dat[Hazard == H,.(Subject,Resp,Stim_1,Stim_2,Stim_3,Stim_4,Stim_5)]
  sdat = dat[Hazard == H,.(Subject,Resp,Stim_1,Stim_2,Stim_3,Stim_4,Stim_5)]
  
  sdat$Stim_1 = scale(sdat$Stim_1)
  sdat$Stim_2 = scale(sdat$Stim_2)
  sdat$Stim_3 = scale(sdat$Stim_3)
  sdat$Stim_4 = scale(sdat$Stim_4)
  sdat$Stim_5 = scale(sdat$Stim_5)
  print(head(sdat))

  s.glme = glmer(Resp~Stim_1+Stim_2+Stim_3+Stim_4+Stim_5+
                   (1|Subject)+(1+Stim_1|Subject)+(1+Stim_2|Subject)+(1+Stim_3|Subject)+(1+Stim_4|Subject)+(1+Stim_5|Subject),
                 sdat,family=binomial,control=glmerControl(calc.derivs=F,optimize='nloptwrap'))

  s.df = coef(s.glme)$Subject
  s.df$Subject = row.names(s.df)
  s.df$Hazard = paste0("H = ",H)
  s.df = merge(s.df,a_all)
  s.long = gather(s.df,TrialBack,Beta,Stim_1:Stim_5)
  s.long$TrialBack = factor(s.long$TrialBack,levels=c('Stim_5','Stim_4','Stim_3','Stim_2','Stim_1'),labels=c('T-5','T-4','T-3','T-2','T-1'))
  s.m = ddply(s.long,.(Hazard,Cx_Terc,TrialBack),summarize,MBeta = mean(Beta),mse = mean_se(Beta)[[2]], pse=mean_se(Beta)[[3]])
  return(list(summary(s.glme),s.long,s.m))  
}

#Takes about 5 minutes to run on a 2017 quadcore macbook pro
library(parallel)
n_cores = 3
c1 = makeCluster(n_cores)
clusterExport(c1,c('glmer','pdat','adaptH_all','sBack_glme','data.table','glmerControl','gather','ddply','.','summarize','mean_se'))
h.past.behav = parLapply(c1,c(.01,.3,.99),function(x) sBack_glme(x,adaptH_all,pdat))
stopCluster(c1)
saveRDS(h.past.behav,'./rds/h_past_behav.rds')
```

```{r}
#Plotting functions
#Logit v trials back
tback_plt_behav = function(past.m,yl,title){
  past.m$Cx_Terc = factor(past.m$Cx_Terc,levels=c('Complexity < .12','.12 < Complexity < .22','.22 < Complexity'))
  plt = ggplot(past.m,aes(TrialBack,MBeta,ymin=mse,ymax=pse,color=Hazard))+
    geom_hline(yintercept=c(0),linetype=2)+
    geom_errorbar(width=0,size=1)+
    geom_line(aes(alpha = Cx_Terc,group=Cx_Terc),size=1)+
    geom_point(aes(group = Cx_Terc),size=4,color='white',alpha=1)+
    geom_point(aes(alpha = Cx_Terc),size=4,stroke=1,shape=16)+
    geom_point(stroke=.75,size=4,shape=1)+
    ylab(yl)+
    xlab('Trials Back')+
    #ggtitle(title)+
    scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
    scale_alpha_manual(values=c(.3,.6,1))+
    facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.title=element_text(color='black',size=16),
      axis.text = element_text(color='black',size=12),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      strip.background = element_blank(),
      strip.text = element_text(size=16,color='black',face='bold'),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    ) 
  print(plt)
  return(plt)
}


#Load previously computed past logits
h.past.behav = readRDS('./rds/h_past_behav.rds')
stim.past.m = rbind(h.past.behav[[1]][[3]],h.past.behav[[2]][[3]],h.past.behav[[3]][[3]])
tback_stim_plt = tback_plt_behav(stim.past.m,'Past Difference Beta', 'Influence of Past Stimuli on Current Response')
ggsave('./Figure5/tback_stim_plt.pdf',plot=tback_stim_plt,width = 9.5,height=4)
```

Figure 5b - sum of past betas vs complexity
```{r}
#Beta sum  v complexity
tback_cor_plt_behav = function(dat,yl,title){
    h1.cor.lv = cor.test(dat$Cx[dat$Hazard == "H = 0.01" & dat$Variability_Group == 'Low Variability'],dat$SBeta[dat$Hazard == "H = 0.01" &dat$Variability_Group == 'Low Variability'],method='spearman')
    h3.cor.lv = cor.test(dat$Cx[dat$Hazard == "H = 0.3"& dat$Variability_Group == 'Low Variability'],dat$SBeta[dat$Hazard == "H = 0.3"& dat$Variability_Group == 'Low Variability'],method='spearman')
    h9.cor.lv = cor.test(dat$Cx[dat$Hazard == "H = 0.99"& dat$Variability_Group == 'Low Variability'],dat$SBeta[dat$Hazard == "H = 0.99"& dat$Variability_Group == 'Low Variability'],method='spearman')
    
    h1.cor.hv = cor.test(dat$Cx[dat$Hazard == "H = 0.01" & dat$Variability_Group == 'High Variability'],dat$SBeta[dat$Hazard == "H = 0.01" &dat$Variability_Group == 'High Variability'],method='spearman')
    h3.cor.hv = cor.test(dat$Cx[dat$Hazard == "H = 0.3"& dat$Variability_Group == 'High Variability'],dat$SBeta[dat$Hazard == "H = 0.3"& dat$Variability_Group == 'High Variability'],method='spearman')
    h9.cor.hv = cor.test(dat$Cx[dat$Hazard == "H = 0.99"& dat$Variability_Group == 'High Variability'],dat$SBeta[dat$Hazard == "H = 0.99"& dat$Variability_Group == 'High Variability'],method='spearman')
    plt = ggplot(dat,aes(Cx,SBeta,color=Hazard))
    
  #Print correlation values
  print('LOW VARIABILITY')
  print(paste0('Low: rho=',h1.cor.lv$estimate[[1]],', p=',h1.cor.lv$p.value[[1]]))
  print(paste0('Intermediate: rho=',h3.cor.lv$estimate[[1]],', p=',h3.cor.lv$p.value[[1]]))
  print(paste0('High: rho=',h9.cor.lv$estimate[[1]],', p=',h9.cor.lv$p.value[[1]]))
  
  print('HIGH VARIABILITY')
  print(paste0('Low: rho=',h1.cor.hv$estimate[[1]],', p=',h1.cor.hv$p.value[[1]]))
  print(paste0('Intermediate: rho=',h3.cor.hv$estimate[[1]],', p=',h3.cor.hv$p.value[[1]]))
  print(paste0('High: rho=',h9.cor.hv$estimate[[1]],', p=',h9.cor.hv$p.value[[1]]))
  
  plt = plt+
    geom_point(size=3.5,shape = 1,stroke=.75)+
    geom_point(size=3.5,shape=16,alpha=.2)+
    geom_point(aes(shape = Variability_Group),size=4)+
    geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
    geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
    ylab(yl)+
    xlab('Complexity')+
    scale_shape_manual(values=c(1,16))+
    scale_linetype_manual(values=c(2,1))+
    scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
    facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      strip.background = element_blank(),
      strip.text = element_text(size=16,color='black',face='bold'),
      axis.title=element_text(color='black',size=16),
      axis.text = element_text(color='black',size=12),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
  return(plt)
}
#Correlation of sum of previous trials (T-2 - T-5) and complexity
h.sum.df.fnc = function(d1,d3,d9){
  h1.sum.df = ddply(subset(d1, TrialBack %in% c('T-2','T-3','T-4','T-5')),.(Subject,Hazard,Variability_Group),summarize,SBeta = sum(abs(Beta)),Cx = mean(Complexity))
  h3.sum.df = ddply(subset(d3, TrialBack %in% c('T-2','T-3','T-4','T-5')),.(Subject,Hazard,Variability_Group),summarize,SBeta = sum(abs(Beta)),Cx = mean(Complexity))
  h9.sum.df = ddply(subset(d9, TrialBack %in% c('T-2','T-3','T-4','T-5')),.(Subject,Hazard,Variability_Group),summarize,SBeta = sum(abs(Beta)),Cx = mean(Complexity))
  return(rbind(h1.sum.df,h3.sum.df,h9.sum.df))
}
h.stim.sum.df = h.sum.df.fnc(h.past.behav[[1]][[2]],h.past.behav[[2]][[2]],h.past.behav[[3]][[2]])
h.stim.sum.corr.plt = tback_cor_plt_behav(h.stim.sum.df,'Sum Absolute Past Beta (T-2 to T-5)','Sum Past Diff Beta v Complexity')
ggsave('./Figure5/sum_paststim_cx_plt.pdf',plot=h.stim.sum.corr.plt,width = 9.5,height=4)
```

Figure 5c - subject hazard by complexity
```{r}
#CLEAN UP THE FITS TO GET RID OF BIASES (i.e., fits for hazard rates that subjects did not see)
h_fits = adaptH_all[,c(2,3,25:27,31:33,114)]
h_fits$NumSess = sapply(1:length(h_fits$Subject), function(x) sum(is.na(h_fits[x,3:5])==0))
for(i in 1:length(h_fits$Subject)){
  if(h_fits$NumSess[i] == 1){
    if(is.na(h_fits$Corr_S1[i])==0){
      h_fits[i,8] = NA
    }
    else if(is.na(h_fits$Corr_S2[i])==0){
      h_fits[i,6] = NA
    }
    else if(is.na(h_fits$Corr_S3[i])==0){
      h_fits[i,7] = NA
    }
  }
}

h_fits.long = gather(h_fits,Hazard,Fit,HNM1_H1:HNM1_H9)
h_fits.long$Hazard = factor(h_fits.long$Hazard,labels = c('H = .01', 'H = .3', 'H = .99'))

#F-tests to capture variance differences
var.test(Fit~Hazard,subset(h_fits.long,Hazard %in% c('H = .01', 'H = .3')))
var.test(Fit~Hazard,subset(h_fits.long,Hazard %in% c('H = .3', 'H = .99')))
var.test(Fit~Hazard,subset(h_fits.long,Hazard %in% c('H = .01', 'H = .99')))
                  

sub_cx_fith_plt = ggplot(h_fits.long,aes(Complexity,Fit,color=Hazard))+
  geom_hline(yintercept=c(0.01),linetype=3,color = "#0072B2")+
  geom_hline(yintercept=c(0.3),linetype=3,color = "#D55E00")+
  geom_hline(yintercept=c(0.99),linetype=3,color = "#E69F00")+
  geom_point(size=4,alpha=.1)+
  geom_point(aes(shape=Variability_Group),size=4,stroke=.75,alpha=1)+
  geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
  scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  scale_shape_manual(values=c(1,16))+
  scale_linetype_manual(values=c(2,1))+
  ylab('Fit Hazard Rate')+
  ylim(c(0,1))+
  facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.text = element_text(size=12,color='black'),
      axis.title = element_text(size=14,color='black'),
      strip.background = element_blank(),
      strip.text = element_text(size=16,color='black',face='bold'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave('./Figure5/sub_cx_fith_plt.pdf',sub_cx_fith_plt,height = 4,width=9.5)
cor.test(h_fits$Complexity[h_fits$Variability_Group=='High Variability'],h_fits$HNM1_H1[h_fits$Variability_Group=='High Variability'],method='spearman')
cor.test(h_fits$Complexity[h_fits$Variability_Group=='Low Variability'],h_fits$HNM1_H1[h_fits$Variability_Group=='Low Variability'],method='spearman')

cor.test(h_fits$Complexity[h_fits$Variability_Group=='High Variability'],h_fits$HNM1_H3[h_fits$Variability_Group=='High Variability'],method='spearman')
cor.test(h_fits$Complexity[h_fits$Variability_Group=='Low Variability'],h_fits$HNM1_H3[h_fits$Variability_Group=='Low Variability'],method='spearman')

cor.test(h_fits$Complexity[h_fits$Variability_Group=='High Variability'],h_fits$HNM1_H9[h_fits$Variability_Group=='High Variability'],method='spearman')
cor.test(h_fits$Complexity[h_fits$Variability_Group=='Low Variability'],h_fits$HNM1_H9[h_fits$Variability_Group=='Low Variability'],method='spearman')
```

Figure 5d - proportion correct by complexity
```{r}
bdat.corr.p = ddply(bdat,.(Subject,H),summarize,Mcor = mean(Correct,na.rm=T),RT = mean(logRT,na.rm=T))
bdat.corr.p = merge(bdat.corr.p,adaptH_all[,c(2,3,114,115)])
bdat.corr.p$Hazard = factor(bdat.corr.p$H,labels = c('H = .01', 'H = .3', 'H = .99'))
bdat.corr.p$Hazard = factor(bdat.corr.p$Hazard,levels = c('H = .3', 'H = .01', 'H = .99'))

bdat.corr.p$Hazard = factor(bdat.corr.p$Hazard,levels=c('H = .01','H = .3','H = .99'))
sub_cx_cor_plt = ggplot(bdat.corr.p,aes(Complexity,Mcor,color=Hazard))+
  geom_hline(yintercept=c(0.93),linetype=3,color = "#0072B2")+
  geom_hline(yintercept=c(0.63),linetype=3,color = "#D55E00")+
  geom_hline(yintercept=c(0.88),linetype=3,color = "#E69F00")+
  geom_point(size=4,alpha=.1)+
  geom_point(aes(shape=Variability_Group),size=4,stroke=.75)+
  geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
  scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
  scale_shape_manual(values=c(1,16))+
  scale_linetype_manual(values=c(2,1))+
  ylim(c(0,1))+
  ylab('Proportion Correct Responses')+
  facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.text = element_text(size=12,color='black'),
      axis.title = element_text(size=14,color='black'),
      strip.background = element_blank(),
      strip.text = element_text(size=16,color='black',face='italic'),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave('./Figure5/sub_cx_cor_plt.pdf',sub_cx_cor_plt,height = 4,width=9.5)
cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .01 & bdat.corr.p$Variability_Group == 'Low Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .01& bdat.corr.p$Variability_Group == 'Low Variability'],method='spearman')
cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .01 & bdat.corr.p$Variability_Group == 'High Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .01& bdat.corr.p$Variability_Group == 'High Variability'],method='spearman')

cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .3& bdat.corr.p$Variability_Group == 'Low Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .3& bdat.corr.p$Variability_Group == 'Low Variability'],method='spearman')
cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .3& bdat.corr.p$Variability_Group == 'High Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .3& bdat.corr.p$Variability_Group == 'High Variability'],method='spearman')

cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .99& bdat.corr.p$Variability_Group == 'Low Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .99& bdat.corr.p$Variability_Group == 'Low Variability'],method='spearman')
cor.test(bdat.corr.p$Complexity[bdat.corr.p$H == .99& bdat.corr.p$Variability_Group == 'High Variability'],bdat.corr.p$Mcor[bdat.corr.p$H == .99& bdat.corr.p$Variability_Group == 'High Variability'],method='spearman')
```





## FIGURE 6

### Figure 6a,b
```{r}
#Compute switch belief

#Offset belief differences for each subject to compute belief switch
for(sub in unique(pdat$Subject)){
  for(sess in unique(pdat$Session)){
    pdat[Subject == sub & Session == sess,("L_Bayes_Prior") := log(pdat[Subject == sub & Session == sess,P_Bayes_Prior])-log(1-pdat[Subject == sub & Session == sess,P_Bayes_Prior])]
    pdat[Subject == sub & Session == sess,("L_Bayes_Prior_plus_1") := c(pdat[Subject == sub & Session == sess,L_Bayes_Prior][2:1000],NA)]
  }
}

# Fucntion to compute belief in switch
getSwBel = function(a,b){
  sw_diff = rep(NA,length(a))
  for(i in 1:length(a)){
    if(is.na(a[i] == T)){
      sw_diff[i] = NA
    }
    else if(sign(a[i]) == 1){
      sw_diff[i] = a[i]-b[i]
    }
    else if(sign(a[i]) == -1){
      sw_diff[i] = b[i]-a[i]
    }
    else if(a[i] == 0){
      sw_diff[i] = 0
    }
  }
  return(sw_diff)
}

# Compute switch belief
for(sub in unique(pdat$Subject)){
  for(sess in unique(pdat$Session)){
    pdat[Subject == sub & Session == sess,("Sw_Belief") := as.numeric(getSwBel(pdat[Subject == sub & Session == sess,L_Bayes_Prior],pdat[Subject == sub & Session == sess,L_Bayes_Prior_plus_1]))]
    pdat[Subject == sub & Session == sess,("Sw_Belief_1") := c(NA,pdat[Subject == sub & Session == sess,Sw_Belief][1:999])]
  }
}

#Get residuals and see how they relate to switch
pdat.resid = pdat[,.(Subject,Hazard,Session,Surprise,Baseline,Change,Baseline_plus_1,Baseline_1,Change_1,StimSwitch,Sw_Belief,Surprise,logRT2)]
pdat.resid[,("Hazard") := factor(pdat.resid$Hazard,levels=c(.01,.3,.99),labels=c('H = .01','H = .3','H = .99'))]
pdat.resid[,("Baseline") := scale(pdat.resid$Baseline)]
pdat.resid[,("Change") := scale(pdat.resid$Change)]
pdat.resid[,("Baseline_1") := scale(pdat.resid$Baseline_1)]
pdat.resid[,("Change_1") := scale(pdat.resid$Change_1)]
pdat.resid[,("Baseline_plus_1") := scale(pdat.resid$Baseline_plus_1)]

resid.base.lme = lmer(Baseline_plus_1~Baseline+Change+(1|Subject)+(1+Baseline|Subject)+(1+Change|Subject),pdat.resid,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
r.b.summ = summary(resid.base.lme)
r.b.idx = as.numeric(names(r.b.summ$resid))
pdat.resid[r.b.idx,('Baseline_resid') := as.numeric(r.b.summ$residuals)]

resid.ec.lme = lmer(Change~Baseline+Change_1+Baseline_1+(1|Subject)+(1+Baseline|Subject)+(1+Change_1|Subject)+(1+Baseline_1|Subject),pdat.resid,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
r.ec.summ = summary(resid.ec.lme)
r.ec.idx = as.numeric(names(r.ec.summ$resid))
pdat.resid[r.ec.idx,('Change_resid') := as.numeric(r.ec.summ$residuals)]


#Difference in belief switch and surprise on switch switch and non-switch trials,per subject by complexity by hazard rate
sup.diff.h = ddply(subset(pdat.resid,is.na(StimSwitch)==0),.(Subject,Hazard,StimSwitch),summarize,
                   Sw_Bel = mean(Sw_Belief,na.rm=T),
                   Sup=mean(Surprise,na.rm=T),
                   Base = mean(Baseline_resid,na.rm=T),
                   Change = mean(Change_resid,na.rm=T))

sw.diff.h.wide = spread(sup.diff.h[,c(1,2,3,4)],StimSwitch,Sw_Bel)
sw.diff.h.wide$SWDiff_SwBel = sw.diff.h.wide$`1`-sw.diff.h.wide$`0`

sup.diff.h.wide = spread(sup.diff.h[,c(1,2,3,5)],StimSwitch,Sup)
sup.diff.h.wide$SWDiff_Sup = sup.diff.h.wide$`1`-sup.diff.h.wide$`0`

b.diff.h.wide = spread(sup.diff.h[,c(1,2,3,6)],StimSwitch,Base)
b.diff.h.wide$SWDiff_Base = b.diff.h.wide$`1`-b.diff.h.wide$`0`

ec.diff.h.wide = spread(sup.diff.h[,c(1,2,3,7)],StimSwitch,Change)
ec.diff.h.wide$SWDiff_Change = ec.diff.h.wide$`1`-ec.diff.h.wide$`0`


#Merge
all.diff.h.wide = merge(sw.diff.h.wide[,c(1,2,5)],sup.diff.h.wide[,c(1,2,5)])
all.diff.h.wide = merge(all.diff.h.wide,b.diff.h.wide[,c(1,2,5)])
all.diff.h.wide = merge(all.diff.h.wide,ec.diff.h.wide[,c(1,2,5)])
all.diff.h.wide = merge(all.diff.h.wide,adaptH_all[,c(2,3,114)])

#Get subject by subject median RTs
pdat.resid$RT = exp(pdat.resid$logRT2)
med.rt = ddply(pdat.resid,.(Subject,Hazard),summarize,MedRT = median(RT,na.rm=T))

all.diff.h.wide = merge(all.diff.h.wide,med.rt)

#Partial correlation function - couldn't get the package to work so sotle the code from https://www.rdocumentation.org/packages/ppcor/versions/1.1/topics/pcor.test
pcor.test <- function(x,y,z,use="mat",method="p",na.rm=T){
	# The partial correlation coefficient between x and y given z
	#
	# pcor.test is free and comes with ABSOLUTELY NO WARRANTY.
	#
	# x and y should be vectors
	#
	# z can be either a vector or a matrix
	#
	# use: There are two methods to calculate the partial correlation coefficient.
	#	 One is by using variance-covariance matrix ("mat") and the other is by using recursive formula ("rec").
	#	 Default is "mat".
	#
	# method: There are three ways to calculate the correlation coefficient, 
	#	    which are Pearson's ("p"), Spearman's ("s"), and Kendall's ("k") methods.
	# 	    The last two methods which are Spearman's and Kendall's coefficient are based on the non-parametric analysis.
	#	    Default is "p".
	#
	# na.rm: If na.rm is T, then all the missing samples are deleted from the whole dataset, which is (x,y,z).
	#        If not, the missing samples will be removed just when the correlation coefficient is calculated.
	#	   However, the number of samples for the p-value is the number of samples after removing 
	#	   all the missing samples from the whole dataset.
	#	   Default is "T".

	x <- c(x)
	y <- c(y)
	z <- as.data.frame(z)

	if(use == "mat"){
		p.use <- "Var-Cov matrix"
		pcor = pcor.mat(x,y,z,method=method,na.rm=na.rm)
	}else if(use == "rec"){
		p.use <- "Recursive formula"
		pcor = pcor.rec(x,y,z,method=method,na.rm=na.rm)
	}else{
		stop("\'use\' should be either \"rec\" or \"mat\"!\n")
	}

	# print the method
	if(gregexpr("p",method)[[1]][1] == 1){
		p.method <- "Pearson"
	}else if(gregexpr("s",method)[[1]][1] == 1){
		p.method <- "Spearman"
	}else if(gregexpr("k",method)[[1]][1] == 1){
		p.method <- "Kendall"
	}else{
		stop("\'method\' should be \"pearson\" or \"spearman\" or \"kendall\"!\n")
	}

	# sample number
	n <- dim(na.omit(data.frame(x,y,z)))[1]
	
	# given variables' number
	gn <- dim(z)[2]

	# p-value
	if(p.method == "Kendall"){
		statistic <- pcor/sqrt(2*(2*(n-gn)+5)/(9*(n-gn)*(n-1-gn)))
		p.value <- 2*pnorm(-abs(statistic))

	}else{
		statistic <- pcor*sqrt((n-2-gn)/(1-pcor^2))
  		p.value <- 2*pnorm(-abs(statistic))
	}

	data.frame(estimate=pcor,p.value=p.value,statistic=statistic,n=n,gn=gn,Method=p.method,Use=p.use)
}			

# By using var-cov matrix
pcor.mat <- function(x,y,z,method="p",na.rm=T){

	x <- c(x)
	y <- c(y)
	z <- as.data.frame(z)

	if(dim(z)[2] == 0){
		stop("There should be given data\n")
	}

	data <- data.frame(x,y,z)

	if(na.rm == T){
		data = na.omit(data)
	}

	xdata <- na.omit(data.frame(data[,c(1,2)]))
	Sxx <- cov(xdata,xdata,m=method)

	xzdata <- na.omit(data)
	xdata <- data.frame(xzdata[,c(1,2)])
	zdata <- data.frame(xzdata[,-c(1,2)])
	Sxz <- cov(xdata,zdata,m=method)

	zdata <- na.omit(data.frame(data[,-c(1,2)]))
	Szz <- cov(zdata,zdata,m=method)

	# is Szz positive definite?
	zz.ev <- eigen(Szz)$values
	if(min(zz.ev)[1]<0){
		stop("\'Szz\' is not positive definite!\n")
	}

	# partial correlation
	Sxx.z <- Sxx - Sxz %*% solve(Szz) %*% t(Sxz)
	
	rxx.z <- cov2cor(Sxx.z)[1,2]

	rxx.z
}

# By using recursive formula
pcor.rec <- function(x,y,z,method="p",na.rm=T){
	# 

	x <- c(x)
	y <- c(y)
	z <- as.data.frame(z)

	if(dim(z)[2] == 0){
		stop("There should be given data\n")
	}

	data <- data.frame(x,y,z)

	if(na.rm == T){
		data = na.omit(data)
	}

	# recursive formula
	if(dim(z)[2] == 1){
		tdata <- na.omit(data.frame(data[,1],data[,2]))
		rxy <- cor(tdata[,1],tdata[,2],m=method)

		tdata <- na.omit(data.frame(data[,1],data[,-c(1,2)]))
		rxz <- cor(tdata[,1],tdata[,2],m=method)

		tdata <- na.omit(data.frame(data[,2],data[,-c(1,2)]))
		ryz <- cor(tdata[,1],tdata[,2],m=method)

		rxy.z <- (rxy - rxz*ryz)/( sqrt(1-rxz^2)*sqrt(1-ryz^2) )
		
		return(rxy.z)
	}else{
		x <- c(data[,1])
		y <- c(data[,2])
		z0 <- c(data[,3])
		zc <- as.data.frame(data[,-c(1,2,3)])

		rxy.zc <- pcor.rec(x,y,zc,method=method,na.rm=na.rm)
		rxz0.zc <- pcor.rec(x,z0,zc,method=method,na.rm=na.rm)
		ryz0.zc <- pcor.rec(y,z0,zc,method=method,na.rm=na.rm)
		
		rxy.z <- (rxy.zc - rxz0.zc*ryz0.zc)/( sqrt(1-rxz0.zc^2)*sqrt(1-ryz0.zc^2) )
		return(rxy.z)
	}			
}		

#Function for plotting and computing correlations
pltCorrs = function(dat,yl,pltTitle,rtpart = F){
  names(dat)[3] = 'Y'
  type = 'spearman'
  if(rtpart == F){
    print('H1')
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .01' & dat$Variability_Group == 'Low Variability'],dat$Y[dat$Hazard == 'H = .01'& dat$Variability_Group == 'Low Variability'],method='spearman'))
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .01' & dat$Variability_Group == 'High Variability'],dat$Y[dat$Hazard == 'H = .01'& dat$Variability_Group == 'High Variability'],method='spearman'))
    print('H3')
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .3'& dat$Variability_Group == 'Low Variability'],dat$Y[dat$Hazard == 'H = .3'& dat$Variability_Group == 'Low Variability'],method='spearman'))
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .3'& dat$Variability_Group == 'High Variability'],dat$Y[dat$Hazard == 'H = .3'& dat$Variability_Group == 'High Variability'],method='spearman'))
    print('H9')
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .99'& dat$Variability_Group == 'Low Variability'],dat$Y[dat$Hazard == 'H = .99'& dat$Variability_Group == 'Low Variability'],method='spearman'))
    print(cor.test(dat$Complexity[dat$Hazard == 'H = .99'& dat$Variability_Group == 'High Variability'],dat$Y[dat$Hazard == 'H = .99'& dat$Variability_Group == 'High Variability'],method='spearman'))
  }
  else{
    # Partial out log RTs
    print('H1')
    h1.dat.lv = subset(dat,Hazard == 'H = .01' & Variability_Group == 'Low Variability' & is.na(MedRT) == 0)
    h1.dat.hv = subset(dat,Hazard == 'H = .01' & Variability_Group == 'High Variability' & is.na(MedRT) == 0)
    print(pcor.test(h1.dat.lv$Complexity,h1.dat.lv$Y,h1.dat.lv$MedRT,method=type))
    print(pcor.test(h1.dat.hv$Complexity,h1.dat.hv$Y,h1.dat.hv$MedRT,method=type))
    print('H3')
    h3.dat.lv = subset(dat,Hazard == 'H = .3' & Variability_Group == 'Low Variability'& is.na(MedRT) == 0)
    h3.dat.hv = subset(dat,Hazard == 'H = .3' & Variability_Group == 'High Variability'& is.na(MedRT) == 0)
    print(pcor.test(h3.dat.lv$Complexity,h3.dat.lv$Y,h3.dat.lv$MedRT,method=type))
    print(pcor.test(h3.dat.hv$Complexity,h3.dat.hv$Y,h3.dat.hv$MedRT,method=type))
    print('H9')
    h9.dat.lv = subset(dat,Hazard == 'H = .99' & Variability_Group == 'Low Variability'& is.na(MedRT) == 0)
    h9.dat.hv = subset(dat,Hazard == 'H = .99' & Variability_Group == 'High Variability'& is.na(MedRT) == 0)
    print(pcor.test(h9.dat.lv$Complexity,h9.dat.lv$Y,h9.dat.lv$MedRT,method='spearman'))
    print(pcor.test(h9.dat.hv$Complexity,h9.dat.hv$Y,h9.dat.hv$MedRT,method='spearman'))  
  }
  plt = ggplot(dat,aes(Complexity,Y,color=Hazard))+
    geom_point(size=4,alpha=.1)+
    geom_point(aes(shape=Variability_Group),size=4)+
    geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
    geom_hline(yintercept=c(0),color='grey20',linetype=3)+
    ylab(yl)+
    xlab('Complexity')+
    scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
    scale_x_continuous(breaks=c(0,.2,.4))+
    scale_shape_manual(values=c(1,16))+
    scale_linetype_manual(values=c(2,1))+
    facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title = element_text(size=14,color='black'),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face='bold'),
      plot.title = element_text(hjust=.5,size=18),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
  print(plt)
  ggsave(paste0('./Figure6/',pltTitle,'.pdf'),plot=plt,height=3.5,width=8)
}
```

```{r}
#SET rtpart = T for partial correlations
#Plot model predictions
pltCorrs(all.diff.h.wide[,c(1,2,3,7,8)],'Switch Belief\nStim Switch minus No-Switch','swBel_sw_h_cx',rtpart = F)
pltCorrs(all.diff.h.wide[,c(1,2,4,7,8)],'Surprise\nStim Switch minus No-Switch','sup_sw_h_cx',rtpart = F)


# Plot baseline and evoked change
pltCorrs(all.diff.h.wide[,c(1,2,5,7,8)],'Baseline Residuals\nStim Switch minus No-Switch','base_sw_h_cx',rtpart = F)
pltCorrs(all.diff.h.wide[,c(1,2,6,7,8)],'Evoked Change Residuals\nStim Switch minus No-Switch','ec_sw_h_cx',rtpart = F)

#Partial correlations between baseline,complexity, and median RTs
h9.dat.lv = subset(all.diff.h.wide,Hazard == 'H = .99' & Variability_Group == 'Low Variability' & is.na(MedRT) == 0)
h9.dat.hv = subset(all.diff.h.wide,Hazard == 'H = .99' & Variability_Group == 'High Variability' & is.na(MedRT) == 0)

print(pcor.test(h9.dat.lv$Complexity,h9.dat.lv$SWDiff_Base,h9.dat.lv$MedRT,method='spearman'))
print(pcor.test(h9.dat.lv$Complexity,h9.dat.lv$SWDiff_Change,h9.dat.lv$MedRT,method='spearman'))

print(pcor.test(h9.dat.lv$MedRT,h9.dat.lv$SWDiff_Base,h9.dat.lv$Complexity,method='spearman'))
print(pcor.test(h9.dat.lv$MedRT,h9.dat.lv$SWDiff_Change,h9.dat.lv$Complexity,method='spearman'))

```







## FIGURE 7
Figures 7a,b - baseline and evoked change residuals vs complexity
```{r}
#Get residuals and see how they relate to switch
pdat.resid = pdat[,.(Subject,Hazard,Baseline,Change, Baseline_1, Change_1,Baseline_plus_1)]
pdat.resid[,("Hazard") := factor(pdat.resid$Hazard,levels=c(.01,.3,.99),labels=c('H = .01','H = .3','H = .99'))]
pdat.resid[,("Baseline") := scale(pdat.resid$Baseline)]
pdat.resid[,("Change") := scale(pdat.resid$Change)]
pdat.resid[,("Baseline_1") := scale(pdat.resid$Baseline_1)]
pdat.resid[,("Change_1") := scale(pdat.resid$Change_1)]
pdat.resid[,("Baseline_plus_1") := scale(pdat.resid$Baseline_plus_1)]

resid.base.lme = lmer(Baseline_plus_1~Baseline+Change+(1|Subject)+(1+Baseline|Subject)+(1+Change|Subject),pdat.resid,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
r.b.summ = summary(resid.base.lme)
r.b.idx = as.numeric(names(r.b.summ$resid))
pdat.resid[r.b.idx,('Baseline_resid') := as.numeric(r.b.summ$residuals)]

resid.ec.lme = lmer(Change~Baseline+Change_1+Baseline_1+(1|Subject)+(1+Baseline|Subject)+(1+Change_1|Subject)+(1+Baseline_1|Subject),pdat.resid,REML=F,control=lmerControl(calc.derivs=F,optimize='nloptwrap'))
r.ec.summ = summary(resid.ec.lme)
r.ec.idx = as.numeric(names(r.ec.summ$resid))
pdat.resid[r.ec.idx,('Change_resid') := as.numeric(r.ec.summ$residuals)]

# RESIDUALS VS COMPLEXITY

#Straight baseline and evoked residuals vs complexity
resid.h = ddply(pdat.resid,.(Subject,Hazard),summarize,Base = mean(Baseline_resid,na.rm=T),Change = mean(Change_resid,na.rm=T))
resid.h = merge(resid.logrt.corr.h,adaptH_all[,c(2,3,114)])

#Function to compute correlations
getCor = function(dat,h,type){
  resid.h1.lv = subset(dat, Hazard == h & Variability_Group == 'Low Variability')
  resid.h1.hv = subset(dat, Hazard == h & Variability_Group == 'High Variability')
  print(h)
  if(type == 'base'){
    print(cor.test(resid.h1.lv$Complexity, resid.h1.lv$Base,method='spearman'))
    print(cor.test(resid.h1.hv$Complexity, resid.h1.hv$Base,method='spearman'))
  }
  else{
    print(cor.test(resid.h1.lv$Complexity, resid.h1.lv$Change,method='spearman'))
    print(cor.test(resid.h1.hv$Complexity, resid.h1.hv$Change,method='spearman'))   
  }
}


#Complexity vs baseline
getCor(resid.h,'H = .01','base')
getCor(resid.h,'H = .3','base')
getCor(resid.h,'H = .99','base')
ggplot(resid.logrt.corr.h,aes(Complexity,Base,color=Hazard))+
    geom_point(size=4,alpha=.1)+
    geom_point(aes(shape=Variability_Group),size=4)+
    geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
    geom_hline(yintercept=c(0),color='grey20',linetype=3)+
    ylab('Mean Baseline Residuals')+
    xlab('Complexity')+
    scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
    scale_x_continuous(breaks=c(0,.2,.4))+
    scale_shape_manual(values=c(1,16))+
    scale_linetype_manual(values=c(2,1))+
    facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title = element_text(size=14,color='black'),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face='bold'),
      plot.title = element_text(hjust=.5,size=18),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave(paste0('/Users/alsfilip/Dropbox/Penn/Auditory_2AFC/Auditory_2AFC/Figures/Fig8_pieces/base_resid_cx_h.pdf'),plot=bse.cx.plt,height=3.5,width=8)

#Complexity vs evoked change
getCor(resid.h,'H = .01','change')
getCor(resid.h,'H = .3','change')
getCor(resid.h,'H = .99','change')
ggplot(resid.logrt.corr.h,aes(Complexity,Change,color=Hazard))+
    geom_point(size=4,alpha=.1)+
    geom_point(aes(shape=Variability_Group),size=4)+
    geom_smooth(aes(linetype=Variability_Group),method='lm',se=F,color='black')+
    geom_hline(yintercept=c(0),color='grey20',linetype=3)+
    ylab('Mean Evoked Change Residuals')+
    xlab('Complexity')+
    scale_color_manual(values=c("#0072B2","#D55E00","#E69F00"))+
    scale_x_continuous(breaks=c(0,.2,.4))+
    scale_shape_manual(values=c(1,16))+
    scale_linetype_manual(values=c(2,1))+
    facet_wrap(~Hazard)+
    theme(
      legend.position = 'none',
      axis.text.x = element_text(size=12,color='black'),
      axis.text.y = element_text(size=12,color='black'),
      axis.title = element_text(size=14,color='black'),
      strip.background = element_blank(),
      strip.text = element_text(size=16,face='bold'),
      plot.title = element_text(hjust=.5,size=18),
      axis.line.x = element_line(size = .5,color="black"),
      axis.line.y = element_line(size=.5,color="black"),
      panel.background = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
    )
ggsave(paste0('/Users/alsfilip/Dropbox/Penn/Auditory_2AFC/Auditory_2AFC/Figures/Fig8_pieces/ec_resid_cx_h.pdf'),plot=ec.cx.plt,height=3.5,width=8)
```

